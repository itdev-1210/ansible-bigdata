---
# ========================
# role-wide parameters
# ========================

# The name of HDFS cluster
# You should use the cluster name or NameNode's URL
#
spark_comm_hdfs_cluster_name: 'mycluster'

# The path of Spark home directory
#
spark_comm_spark_home: '/usr/local/spark/default'

# The path to unarchive Spark packages
#
spark_comm_dir: '/usr/local/spark'

# The maximum number of executers
spark_comm_exe_instances: '2'

# The base of URL and pacakge name of Spark
# You should specify the following parameters by your self

#spark_comm_package_url_base: ''
#spark_comm_package_name: ''

# You should configure the path of metrics properties to be read
# by Driver and Executors, if necessary
#spark_comm_metrics_properties_path: '/etc/spark/conf/metrics.properties'


# ====================================
# Each configuration file parameters
# ====================================

# spark-defaults.conf
spark_comm_defaults_tmpl: 'spark-defaults.conf.j2'
spark_comm_master: 'yarn' 
spark_comm_logdir: 'hdfs://{{ spark_comm_hdfs_cluster_name }}/var/log/spark'
spark_comm_historyserver: '{{ groups["hadoop_other"][0] }}'
spark_comm_driver_extralib: '/usr/lib/hadoop/lib/native/'
spark_comm_executor_extralib: '/usr/lib/hadoop/lib/native/'
# spark_comm_executor_memOverhead: '2G'
# spark_comm_driver_extrajavaopts: '-XX:+UseG1GC -XX:MaxMetaspaceSize=256M'
# spark_comm_executor_extrajavaopts: '-XX:+UseG1GC -XX:MaxMetaspaceSize=256M'

# hive-site.xml

spark_comm_hive_site_tmpl: 'hive-site.xml.j2'

# The following parameters are needed to connect the remote metastore serfvice

#spark_comm_hive_meta_uri: 'thrift://localhost:9083'

# You can configure warehouse dir

#saprk_comm_hive_warehouse: '/user/hive/warehouse'

# spark-env.sh
spark_comm_env_tmpl: 'spark-env.sh.j2'
spark_comm_exe_cores: '1'
spark_comm_exe_mem: '400M'
spark_comm_dri_mem: '400M'
spark_comm_history_dir: 'hdfs://{{ spark_comm_hdfs_cluster_name }}/var/log/spark'
spark_comm_scala_home: '/usr/local/scala/default'

# /etc/spark/conf/metrics.properties
spark_comm_metrics_tmpl: 'metrics.properties.j2'
spark_comm_graphite_enabled: True
spark_comm_graphite_host: '{{ groups["hadoop_other"] | join(",") }}'
spark_comm_graphite_prefix: 'spark'
spark_comm_graphite_port: '2003'

# /etc/spark/conf/log4j-spark.properties
spark_comm_log4j_spark_properties_tmpl: 'log4j-spark.properties.j2'
spark_comm_log4j_rootCategory: 'WARN, rolling'
spark_comm_log4j_filename: 'spark.log'
spark_comm_log4j_ConversionPattern: '%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n'
spark_comm_log4j_MaxFileSize: '256Mb'
spark_comm_log4j_MaxBackupIndex: '10'
